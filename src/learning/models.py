import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
import pytorch_lightning as pl
import os 
import shutil
import warnings

import wandb
import nflows.nn.nets
from nflows.distributions.normal import StandardNormal
from nflows.flows import Flow
from nflows.transforms import CompositeTransform, ReversePermutation
from nflows.transforms.coupling import AffineCouplingTransform
from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform

import utils
import utils.general
import utils.logging
import policies.costs
from policies.mppi import MPPIComputer
import policies.samplers
from visual import Visual

class PolicyFlowActionDistribution(pl.LightningModule):
    """
    Learns the optimal action distribution for MPPI using normalizing flows

    When used, an object of this class will be used in tandem with a MPPI policy 
    object and this will be used to generate optimal actions
    """
    def __init__(
        self, 
        dynamics,
        K,
        H,
        lambda_,
        map_,
        context_input_size,
        context_output_size,
        num_flow_layers,
        learning_rate,
        log_folder,
    ):
        # Initialize the parent class (pl.LightningModule)
        super(PolicyFlowActionDistribution, self).__init__()

        # Don't defaultly set a goal, this needs to be set 
        self.state_goal = None

        # Save the learning rate
        self.learning_rate = learning_rate

        # Will need access to an MPPI computer
        self.mppi_computer = MPPIComputer(
            dynamics=dynamics,
            K=K,
            H=H,
            lambda_=lambda_,
            map_=map_,
        )

        # Define the internal shapes
        self.K = K
        self.H = H
        self.context_input_size = context_input_size
        self.context_output_size = context_output_size
        self.num_flow_layers = num_flow_layers

        # Create the learning network
        self.model = self._create_model()

        # Dummy variable for device checking
        self.dummy = nn.Parameter(torch.zeros(1))

        # No logging by default
        self.log_folder = log_folder
        self.policy_mppi_folder = os.path.join(self.log_folder, "policy", "mppi")
        os.makedirs(self.policy_mppi_folder, exist_ok=True)
        self.logging_enabled = False

        # Print out model architecture information
        print(f"{self.__class__.__name__} initialized:")
        print(f"\t-K={K}")
        print(f"\t-H={H}")
        print(f"\t-context_input_size={context_input_size}")
        print(f"\t-context_output_size={context_output_size}")
        print(f"\t-num_flow_layers={num_flow_layers}")
        print(f"\t-learning_rate={learning_rate}")

    def device(self):
        return self.dummy.device
        
    def update_state_goal(
        self,
        state_goal,
    ):
        self.state_goal = state_goal

    def update_logging_enabled(self, logging_enabled):
        self.logging_enabled = logging_enabled

    def delete_logs(self):
        """
        Delete all logs
        """
        if os.path.exists(self.policy_mppi_folder):
            print(f"Deleting logs in {self.policy_mppi_folder}")
            shutil.rmtree(self.policy_mppi_folder)

    def _create_model(self):
        """
        Define a normalizing flow network that takes a sample from a 
        standard normal distribution and transforms it into a sample
        from the learned optimal action distribution

        When we sample from the learned optimal action distribution,
        we get a tensor with enough material in it to reshape into
        (H, |A|), where H is the horizon and |A| is the dimension of 
        the action space

        The flow should be parameterized by a fixed length context vector
        which is generated by a separate context network
        """

        # Convienience and readability
        A = self.mppi_computer.dynamics.action_size()
        H = self.H
        C = self.context_output_size
        #print(f"Flow network: A={A}, H={H}, C={C}")

        def create_net(in_features, out_features):
            # Make a little baby resnet
            net = nflows.nn.nets.ResidualNet(
                in_features,
                out_features, 
                context_features=C,
                hidden_features=256, 
                num_blocks=8,
                use_batch_norm=False,
            )
            return net

        transforms = []
        for _ in range(self.num_flow_layers):
            # https://github.com/bayesiains/nflows/blob/3b122e5bbc14ed196301969c12d1c2d94fdfba47/nflows/transforms/coupling.py#L212
            affine_coupling_transform = AffineCouplingTransform(
                # A simple alternating binary mask stating which features will be transformed
                # via an affine transformation (mask > 0) and which will inform the creation of the
                # the transformation parameters (mask <= 0, identity)
                mask=torch.arange(H*A) % 2,  
                # A function that creates a transformation object that which is constructed with two
                # constructor variables: the number of identity features, and the number of transform features
                # Moreover, it must have a forward function which accepts the parts of the input that are to 
                # be transformed, and the context vector, and produces some transformation parameters that will
                # be a
                #transform_net_create_fn=self._create_transform_parameters_block,
                transform_net_create_fn=create_net,
            )
            transforms.append(affine_coupling_transform)

            # Reverses the elements of a 1d input
            # https://github.com/bayesiains/nflows/blob/3b122e5bbc14ed196301969c12d1c2d94fdfba47/nflows/transforms/permutations.py#L56
            reverse_permutation = ReversePermutation(features=H*A)
            transforms.append(reverse_permutation)

        # Need a context network
        context_hidden_size = 256
        context_network = nn.Sequential(
            nn.Linear(self.context_input_size, context_hidden_size),
            nn.ReLU(),
            nn.Linear(context_hidden_size, context_hidden_size),
            nn.ReLU(),
            nn.Linear(context_hidden_size, self.context_output_size)
        )

        # Convert into a single composite transform
        transform = CompositeTransform(transforms)
        distribution = StandardNormal(shape=[H*A])
        flow = Flow(transform, distribution, embedding_net=context_network)
        return flow
    
    def forward(self, state_history, action_history, state_goal, force_no_logging=False):

        # Convienience variables
        B = state_history.shape[0]
        S = self.mppi_computer.dynamics.state_size()
        A = self.mppi_computer.dynamics.action_size()
        H = self.H
        L = state_history.shape[1] # Lookback
        K = self.K
        C = self.context_output_size

        # TODO: expand to handle batched inputs
        # Batch size should only ever be 1 at this time
        if B != 1:
            raise ValueError(f"Batch size (first dimension) must be 1, but is {B}")

        # Assert that the histories and goals have a batch dimension
        assert (state_history.shape[0] == B and action_history.shape[0] == B and state_goal.shape[0] == B), "Expected batch dimension to be first and equal for all inputs"    
        assert state_history.shape == (B, L, S), f"Expected state_history to have shape (B={B}, L={L}, |S|={S}), got {state_history.shape}"
        assert action_history.shape == (B, L-1, A), f"Expected action_history to have shape (B={B}, L={L-1}, |A|={A}), got {action_history.shape}"
        assert state_goal.shape == (B, S), f"Expected state_goal to have shape (B={B}, |S|={S}), got {state_goal.shape}"

        # Tensorize if they are not already tensors 
        if not torch.is_tensor(state_history):
            state_history = torch.tensor(state_history, dtype=torch.float32)
        if not torch.is_tensor(action_history):
            action_history = torch.tensor(action_history, dtype=torch.float32)
        if not torch.is_tensor(state_goal):
            state_goal = torch.tensor(state_goal, dtype=torch.float32)

        # Assemble the context vector, which includes the
        # current state
        # goal state
        context_input = torch.cat((
            state_history[:,-1,:], 
            state_goal
        ), dim=-1)

        # The context shape is (batch_size, context_size) at this point
        assert context_input.shape == (B, self.context_input_size), f"Expected context vector to have shape (B={B}, |C|={C}), got {context_input.shape}"

        # TODO there could be something cool here with multiple context vectors
        # representing different things

        # Ensure there are no nans in the context
        if torch.isnan(context_input).any():
            raise ValueError(f"Context vector contains NaNs")

        # Sample from the normalizing flow and get log probs (this is more
        # efficient than sampling and then computing the log probs, though log
        # likelihoods are not needed during validation)
        # We'll get actions as (batch_size, K, H*A) 
        # We'll get log_p_likelihoods as (batch_size, K)
        # Read this as 'generate K samples for each context vector', where we only have one
        # context vector
        samples, log_p_likelihoods = self.model.sample_and_log_prob(K, context=context_input)

        # Are there any nans in the result?
        if torch.isnan(samples).any():
            raise ValueError(f"Normalizing flow samples contain NaNs")
        if torch.isnan(log_p_likelihoods).any():
            raise ValueError(f"Normalizing flow log likelihoods contain NaNs")

        # Do some checks
        assert samples.shape == (B, K, H*A), f"Expected action_plans to have shape (B={B}, K={K}, H*|A|={H*A}), got {samples.shape}"
        assert log_p_likelihoods.shape == (B, K), f"Expected log_p_likelihoods to have shape (B={B}, K={K}), got {log_p_likelihoods.shape}"

        # Reshape the batch of samples into B lots of K, H-long sequences of action vectors
        action_plans = samples.view(B, K, H, A)

        # To get a loss we'll need to see what trajectories these action plans
        # result it, and score them, but we can't assume that dynamics and 
        # rewards are differentiable, so we'll need to compute the trajectories
        # and scores without grads (grad information will still be present in the
        # conditional variational posterior)

        # Copy the action plans and numpy-ify them
        action_plans = action_plans[0].detach().cpu().numpy()

        # Action plans need to be in the allowed ranges (if not then clip)
        action_ranges = self.mppi_computer.dynamics.action_ranges()
        action_plans = np.clip(action_plans, action_ranges[:,0], action_ranges[:,1])

        # Make sure the action plans have no nans
        if np.isnan(action_plans).any():
            raise ValueError("Action plans contain NaNs")

        # Compute the optimal future using MPPI
        # Recall that batches are of size 1
        state_history = state_history[0].detach().cpu().numpy()
        action_history = action_history[0].detach().cpu().numpy()
        state_goal = state_goal[0].detach().cpu().numpy()
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan = self.mppi_computer.compute(
            state_history,
            action_history,
            state_goal,
            # The MPPI computer needs to take a 'sampler'
            policies.samplers.FixedSampler(action_plans),
        )    

        # If logging is enabled then provide it
        if self.logging_enabled and not force_no_logging:
            self._log_helper(
                state_history, action_history, state_goal,
                state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, 
                log_p_likelihoods
            )

        return state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, log_p_likelihoods
    
    def _log_helper(
        self, 
        state_history, action_history, state_goal,
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, 
        log_p_likelihoods
    ):

        # Create a subfolder for this step
        folder = os.path.join(self.policy_mppi_folder, f"step_{utils.general.get_timestamp(ultra_precise=True)}")
        os.makedirs(folder, exist_ok=True)

        # Save the state and action plans
        utils.logging.save_state_and_action_trajectories(
            folder,
            state_plans,
            action_plans,
        )

        # Save the costs
        utils.logging.pickle_to_filepath(
            os.path.join(folder, "costs.pkl"),
            costs,
        )

        # If we're logging we will want to see what the optimal plan was
        optimal_state_plan = np.zeros((self.mppi_computer.H, self.mppi_computer.dynamics.state_size()))
        for h in range(self.mppi_computer.H):
            optimal_state_plan[h] = self.mppi_computer.dynamics.step(
                state_history[-1] if h == 0 else optimal_state_plan[h - 1],
                optimal_action_plan[h],
            )

        # Save the optimal plans
        utils.logging.save_state_and_action_trajectories(
            folder,
            optimal_state_plan,
            optimal_action_plan,
            suffix="optimal",
        )

    def act(
        self,
        state_history,
        action_history,
    ):
        # Check if we have a goal
        if self.state_goal is None:
            raise ValueError("A goal state must be set before acting")
        
        # This may get called from wherever, so we'll need to make sure
        # we're on the right device
        device = self.device()
        state_history  = torch.tensor(state_history, dtype=torch.float32).to(device)
        action_history = torch.tensor(action_history, dtype=torch.float32).to(device)
        state_goal     = torch.tensor(self.state_goal, dtype=torch.float32).to(device)
        
        # Return the optimal action
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, log_likelihoods = self.forward(
            # Batch size of 1
            torch.unsqueeze(state_history, 0),
            torch.unsqueeze(action_history, 0),
            torch.unsqueeze(state_goal, 0),
            force_no_logging=True,
        )    

        # Back to numpy
        state_history = state_history.detach().cpu().numpy()
        action_history = action_history.detach().cpu().numpy()
        state_goal = state_goal.detach().cpu().numpy()

        return optimal_action_plan[0]
    
    def generic_step(self, batch, batch_idx, stage):
        """
        Generate K samples from the learned optimal action distribution,
        and compute a loss that encourages the samples to be close to
        the optimal action distribution
        """

        # Start by getting the contextual variables
        state_history = batch["state_history"]
        action_history = batch["action_history"]
        state_goal = batch["state_goal"]

        # Check the done flag, if it's done then complete this training/validation stage
        # TODO only works for batch size = 1
        if batch["done_flag"][0]: # and stage == "val":
            raise StopIteration(f"Environment is done, flag: {batch['done_message']}")

        # What is the batch size?
        B = state_history.shape[0]

        # Run a pass, getting all the required information 
        # from the MPPI computer
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, log_p_likelihoods = self.forward(
            # Batch size of 1
            state_history,
            action_history,
            state_goal
        )  
        
        # Note that after this call everything except log_p_likelihoods will be numpy.
        # log_p_likelihoods will be a torch tensor, with grad information

        # We'll do everything with log probabilities for stability
        # Costs. Note that we have the probability of a trajectory being optimal 
        # is estimated by the exponential of the negative cost: p(o|tau) = exp(-cost(tau)),
        # meaning that the log probability of optimality is -cost(tau)
        costs = torch.tensor(costs, dtype=torch.float32)
        costs = costs.to(log_p_likelihoods.device)
        log_p_optimality = -costs

        # Here we define some hyperparameters that balance exploration and exploitation
        # exploitation refers to optimality, and exploration refers to the entropy of the
        # action distribution
        # If the following variable is large, then the probability of optimality is 
        # considered more so than the spread of the action distribution when computing
        # the loss
        prefer_optimality_over_entropy = 0.5

        # Addition in log space is multiplication in normal space
        # Multiplication in log space is exponentiation in normal space
        log_p_combined = prefer_optimality_over_entropy * log_p_optimality + log_p_likelihoods

        # What fraction of each samples combined probability does
        # each sample take up?
        # In normal space this would be p_combined / mean(p_combined)
        # In log space this becomes log(p_combined) - log(mean(p_combined))
        # Note that softmax is = exp(x) / sum(exp(x)), so when x = log_p_combined
        # we end up converting back to normal space and doing the normal space
        # calculation
        # Recall that dim here makes every slice along the dim sum to 1, so this
        # needs to be across all samples
        weights = torch.softmax(log_p_combined, dim=1)

        # Assert the shape of weights
        assert weights.shape == (B, self.K), f"Expected weights to have shape (B={B}, K={self.K}), got {weights.shape}"
        # Check for nans
        assert not torch.isnan(weights).any(), f"Weights contain {torch.sum(torch.isnan(weights)).item()}/{B*self.K} NaNs"

        # Assemble the total loss
        loss = -torch.mean(weights * log_p_likelihoods)

        # Do custom logging
        self._log_scalar_per_step(f'{stage}/loss', loss)
        self._log_scalar_per_step(f'{stage}/cost/min', torch.min(costs))
        self._log_scalar_per_step(f'{stage}/cost/mean', torch.mean(costs))
        #self._log_scalar_per_step(f'{stage}/cost/max', torch.max(costs))

        # And return the loss
        return loss
    
    def _log_scalar_per_step(self, name, value):
        B = 1
        self.log(name, value, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=B)

    def _log_probability_distribution(self, name, distribution):
        self.logger.experiment.log({name: wandb.Histogram(distribution)}, step=self.global_step)
    
    def training_step(self, batch, batch_idx):
        return self.generic_step(batch, batch_idx, 'train')

    def validation_step(self, batch, batch_idx):
        return self.generic_step(batch, batch_idx, 'val')

    def on_validation_epoch_start(self):
        self.update_logging_enabled(True)

    def on_validation_epoch_end(self):

        # If the log folder does not have more than 10 steps in it, return (i.e. when we're doing
        # validation sanity checks)
        if len(os.listdir(self.policy_mppi_folder)) < 10:
            warnings.warn("Not enough steps to create a video - are you running a validation sanity check?")
        
        else:

            # # Get the agent and environment to log
            # self.trainer.datamodule.dataset_val.agent.log(self.log_folder)
            # self.trainer.datamodule.dataset_val.environment.log(self.log_folder)

            # print(len(self.trainer.datamodule.dataset_val.agent.state_history_tracker))
            # print(len(self.trainer.datamodule.dataset_val.agent.action_history_tracker))
            # print(len(self.trainer.datamodule.dataset_val.environment.state_history_tracker))
            # print(len(self.trainer.datamodule.dataset_val.environment.action_history_tracker))

            # Create visualization
            visual = Visual(self.log_folder)
            #visual.plot_histories()
            video_filepath = visual.render_video(desired_fps=25)

            # Upload the video to wandb and delete the file
            wandb.log({"video": wandb.Video(video_filepath)}, step=self.global_step)
            os.remove(video_filepath)

        # Delete the logs
        self.delete_logs()

        # Don't log outside of validation
        self.update_logging_enabled(False)
        
    def configure_optimizers(self):
        learnable_parameters = self.model.parameters()
        return optim.Adam(learnable_parameters, lr=self.learning_rate)