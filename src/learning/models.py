import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
import pytorch_lightning as pl
import os 
import shutil
import warnings
import matplotlib.pyplot as plt
import tempfile

import wandb
import nflows.nn.nets
from nflows.distributions.normal import StandardNormal
from nflows.flows import Flow
from nflows.transforms import CompositeTransform, ReversePermutation
from nflows.transforms.coupling import AffineCouplingTransform
from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform

import utils
import utils.general
import utils.logging
import policies.costs
from policies.mppi import MPPIComputer
import policies.samplers
from visual import Visual

from environment import Environment

class PolicyFlowActionDistribution(pl.LightningModule):
    """
    Learns the optimal action distribution for MPPI using normalizing flows

    When used, an object of this class will be used in tandem with a MPPI policy 
    object and this will be used to generate optimal actions
    """
    def __init__(
        self, 
        dynamics,
        K,
        H,
        lambda_,
        map_,
        context_input_size,
        context_output_size,
        num_flow_layers,
        learning_rate,
        environment,
        log_folder,
    ):
        # Initialize the parent class (pl.LightningModule)
        super(PolicyFlowActionDistribution, self).__init__()

        # Don't defaultly set a goal, this needs to be set 
        self.state_goal = None

        # Save the learning rate
        self.learning_rate = learning_rate

        # Will need access to an MPPI computer
        self.mppi_computer = MPPIComputer(
            dynamics=dynamics,
            K=K,
            H=H,
            lambda_=lambda_,
            map_=map_,
        )

        # Define the internal shapes
        self.K = K
        self.H = H
        self.context_input_size = context_input_size
        self.context_output_size = context_output_size
        self.num_flow_layers = num_flow_layers

        # Create the learning network
        self.model = self._create_model()

        # Dummy variable for device checking
        self.dummy = nn.Parameter(torch.zeros(1))

        # No logging by default
        self.log_folder = log_folder
        self.policy_mppi_folder = os.path.join(self.log_folder, "policy", "mppi")
        os.makedirs(self.policy_mppi_folder, exist_ok=True)
        self.logging_enabled = False

        # Need an access to environment during training
        self.environment = environment
        self._environment_reset()

        # Print out model architecture information
        print(f"{self.__class__.__name__} initialized:")
        print(f"\t-K={K}")
        print(f"\t-H={H}")
        print(f"\t-context_input_size={context_input_size}")
        print(f"\t-context_output_size={context_output_size}")
        print(f"\t-num_flow_layers={num_flow_layers}")
        print(f"\t-learning_rate={learning_rate}")

    def device(self):
        return self.dummy.device
        
    def update_state_goal(
        self,
        state_goal,
    ):
        self.state_goal = state_goal

    def update_logging_enabled(self, logging_enabled):
        self.logging_enabled = logging_enabled

    def delete_logs(self):
        """
        Delete all logs
        """
        if os.path.exists(self.policy_mppi_folder):
            print(f"Deleting logs in {self.policy_mppi_folder}")
            shutil.rmtree(self.policy_mppi_folder)

    def _create_model(self):
        """
        Define a normalizing flow network that takes a sample from a 
        standard normal distribution and transforms it into a sample
        from the learned optimal action distribution

        When we sample from the learned optimal action distribution,
        we get a tensor with enough material in it to reshape into
        (H, |A|), where H is the horizon and |A| is the dimension of 
        the action space

        The flow should be parameterized by a fixed length context vector
        which is generated by a separate context network
        """

        # Convienience and readability
        A = self.mppi_computer.dynamics.action_size()
        H = self.H
        C = self.context_output_size
        #print(f"Flow network: A={A}, H={H}, C={C}")

        def create_net(in_features, out_features):
            # Make a little baby resnet
            net = nflows.nn.nets.ResidualNet(
                in_features,
                out_features, 
                context_features=C,
                hidden_features=256, 
                num_blocks=8,
                use_batch_norm=False,
            )
            return net

        transforms = []
        for _ in range(self.num_flow_layers):
            # https://github.com/bayesiains/nflows/blob/3b122e5bbc14ed196301969c12d1c2d94fdfba47/nflows/transforms/coupling.py#L212
            affine_coupling_transform = AffineCouplingTransform(
                # A simple alternating binary mask stating which features will be transformed
                # via an affine transformation (mask > 0) and which will inform the creation of the
                # the transformation parameters (mask <= 0, identity)
                mask=torch.arange(H*A) % 2,  
                # A function that creates a transformation object that which is constructed with two
                # constructor variables: the number of identity features, and the number of transform features
                # Moreover, it must have a forward function which accepts the parts of the input that are to 
                # be transformed, and the context vector, and produces some transformation parameters that will
                # be a
                #transform_net_create_fn=self._create_transform_parameters_block,
                transform_net_create_fn=create_net,
            )
            transforms.append(affine_coupling_transform)

            # Reverses the elements of a 1d input
            # https://github.com/bayesiains/nflows/blob/3b122e5bbc14ed196301969c12d1c2d94fdfba47/nflows/transforms/permutations.py#L56
            reverse_permutation = ReversePermutation(features=H*A)
            transforms.append(reverse_permutation)

        # Need a context network
        context_hidden_size = 256
        context_network = nn.Sequential(
            nn.Linear(self.context_input_size, context_hidden_size),
            nn.ReLU(),
            nn.Linear(context_hidden_size, context_hidden_size),
            nn.ReLU(),
            nn.Linear(context_hidden_size, self.context_output_size)
        )

        # Convert into a single composite transform
        transform = CompositeTransform(transforms)
        distribution = StandardNormal(shape=[H*A])
        flow = Flow(transform, distribution, embedding_net=context_network)
        return flow
    
    def forward(self, state_history, action_history, state_goal, force_no_logging=False):

        # Convienience variables
        B = state_history.shape[0]
        S = self.mppi_computer.dynamics.state_size()
        A = self.mppi_computer.dynamics.action_size()
        H = self.H
        L = state_history.shape[1] # Lookback
        K = self.K
        C = self.context_output_size

        # TODO: expand to handle batched inputs
        # Batch size should only ever be 1 at this time
        if B != 1:
            raise ValueError(f"Batch size (first dimension) must be 1, but is {B}")

        # Assert that the histories and goals have a batch dimension
        assert (state_history.shape[0] == B and action_history.shape[0] == B and state_goal.shape[0] == B), "Expected batch dimension to be first and equal for all inputs"    
        assert state_history.shape == (B, L, S), f"Expected state_history to have shape (B={B}, L={L}, |S|={S}), got {state_history.shape}"
        assert action_history.shape == (B, L-1, A), f"Expected action_history to have shape (B={B}, L={L-1}, |A|={A}), got {action_history.shape}"
        assert state_goal.shape == (B, S), f"Expected state_goal to have shape (B={B}, |S|={S}), got {state_goal.shape}"

        # Tensorize if they are not already tensors 
        if not torch.is_tensor(state_history):
            state_history = torch.tensor(state_history, dtype=torch.float32)
        if not torch.is_tensor(action_history):
            action_history = torch.tensor(action_history, dtype=torch.float32)
        if not torch.is_tensor(state_goal):
            state_goal = torch.tensor(state_goal, dtype=torch.float32)

        # Assemble the context vector, which includes the
        # current state
        # goal state
        context_input = torch.cat((
            state_history[:,-1,:], 
            state_goal
        ), dim=-1)

        # The context shape is (batch_size, context_size) at this point
        assert context_input.shape == (B, self.context_input_size), f"Expected context vector to have shape (B={B}, |C|={C}), got {context_input.shape}"

        # TODO there could be something cool here with multiple context vectors
        # representing different things

        # Ensure there are no nans in the context
        if torch.isnan(context_input).any():
            raise ValueError(f"Context vector contains NaNs")

        # Sample from the normalizing flow and get log probs (this is more
        # efficient than sampling and then computing the log probs, though log
        # likelihoods are not needed during validation)
        # We'll get actions as (batch_size, K, H*A) 
        # We'll get log_p_likelihoods as (batch_size, K)
        # Read this as 'generate K samples for each context vector', where we only have one
        # context vector
        samples, log_p_likelihoods = self.model.sample_and_log_prob(K, context=context_input)

        # Are there any nans in the result?
        if torch.isnan(samples).any():
            raise ValueError(f"Normalizing flow samples contain NaNs")
        if torch.isnan(log_p_likelihoods).any():
            raise ValueError(f"Normalizing flow log likelihoods contain NaNs")

        # Do some checks
        assert samples.shape == (B, K, H*A), f"Expected action_plans to have shape (B={B}, K={K}, H*|A|={H*A}), got {samples.shape}"
        assert log_p_likelihoods.shape == (B, K), f"Expected log_p_likelihoods to have shape (B={B}, K={K}), got {log_p_likelihoods.shape}"

        # Reshape the batch of samples into B lots of K, H-long sequences of action vectors
        action_plans = samples.view(B, K, H, A)

        # To get a loss we'll need to see what trajectories these action plans
        # result it, and score them, but we can't assume that dynamics and 
        # rewards are differentiable, so we'll need to compute the trajectories
        # and scores without grads (grad information will still be present in the
        # conditional variational posterior)

        # Copy the action plans and numpy-ify them
        action_plans = action_plans[0].detach().cpu().numpy()

        # Action plans need to be in the allowed ranges (if not then clip)
        action_ranges = self.mppi_computer.dynamics.action_ranges()
        action_plans = np.clip(action_plans, action_ranges[:,0], action_ranges[:,1])

        # Make sure the action plans have no nans
        if np.isnan(action_plans).any():
            raise ValueError("Action plans contain NaNs")

        # Compute the optimal future using MPPI
        # Recall that batches are of size 1
        state_history = state_history[0].detach().cpu().numpy()
        action_history = action_history[0].detach().cpu().numpy()
        state_goal = state_goal[0].detach().cpu().numpy()
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan = self.mppi_computer.compute(
            state_history,
            action_history,
            state_goal,
            # The MPPI computer needs to take a 'sampler'
            policies.samplers.FixedSampler(action_plans),
        )    

        # If logging is enabled then provide it
        if self.logging_enabled and not force_no_logging:
            self._log_helper(
                state_history, action_history, state_goal,
                state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, 
                log_p_likelihoods
            )

        return state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, log_p_likelihoods
    
    def _log_helper(
        self, 
        state_history, action_history, state_goal,
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, 
        log_p_likelihoods
    ):

        # Create a subfolder for this step
        try:
            num_in_folder = len(os.listdir(self.policy_mppi_folder))
        except FileNotFoundError:
            num_in_folder = 0
        folder = os.path.join(self.policy_mppi_folder, f"step-{utils.general.get_timestamp(ultra_precise=True)}")
        os.makedirs(folder, exist_ok=True)

        # Save the state and action plans
        utils.logging.save_state_and_action_trajectories(
            folder,
            state_plans,
            action_plans,
        )

        # Save the costs
        utils.logging.pickle_to_filepath(
            os.path.join(folder, "costs.pkl"),
            costs,
        )

        # If we're logging we will want to see what the optimal plan was
        optimal_state_plan = np.zeros((self.mppi_computer.H, self.mppi_computer.dynamics.state_size()))
        for h in range(self.mppi_computer.H):
            optimal_state_plan[h] = self.mppi_computer.dynamics.step(
                state_history[-1] if h == 0 else optimal_state_plan[h - 1],
                optimal_action_plan[h],
            )

        # Save the optimal plans
        utils.logging.save_state_and_action_trajectories(
            folder,
            optimal_state_plan,
            optimal_action_plan,
            suffix="optimal",
        )

    def act(
        self,
        state_history,
        action_history,
    ):
        # Check if we have a goal
        if self.state_goal is None:
            raise ValueError("A goal state must be set before acting")
        
        # This may get called from wherever, so we'll need to make sure
        # we're on the right device
        device = self.device()
        state_history  = torch.tensor(state_history, dtype=torch.float32).to(device)
        action_history = torch.tensor(action_history, dtype=torch.float32).to(device)
        state_goal     = torch.tensor(self.state_goal, dtype=torch.float32).to(device)
        
        # Return the optimal action
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, log_likelihoods = self.forward(
            # Batch size of 1
            torch.unsqueeze(state_history, 0),
            torch.unsqueeze(action_history, 0),
            torch.unsqueeze(state_goal, 0),
            force_no_logging=True,
        )    

        # Back to numpy
        state_history = state_history.detach().cpu().numpy()
        action_history = action_history.detach().cpu().numpy()
        state_goal = state_goal.detach().cpu().numpy()

        return optimal_action_plan[0]

    def _environment_reset(self):
        # If the log folder is set, we log the state and action trajectories
        # Note that this overwrites the old
        if self.log_folder is not None:
            self.environment.log(self.log_folder)

        # Generate the task that this episode represents
        state_initial, state_goal = Environment.get_two_states_separated_by_distance(
            self.environment.map,
            min_distance=26,
            rng=utils.general.get_time_based_rng(),
        )

        # Reset the environment
        self.environment.reset(state_initial, state_goal)

    def generic_step(self, batch, batch_idx, stage):
        """
        Generate K samples from the learned optimal action distribution,
        and compute a loss that encourages the samples to be close to
        the optimal action distribution
        """

        # Start by getting the context variables
        state_initial = self.environment.state_history_tracker.get_first_item()
        state_goal    = self.environment.state_goal

        # And other policy inputs
        state_history  = self.environment.state_history_tracker.get_last_n_items_with_zero_pad(self.H)
        action_history = self.environment.action_history_tracker.get_last_n_items_with_zero_pad(self.H-1)

        # What is the batch size?
        B = 1

        # Run a pass, getting all the required information 
        # from the MPPI computer
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, log_p_likelihoods = self.forward(
            # Batch size of 1
            torch.unsqueeze(torch.tensor(state_history, dtype=torch.float32), 0).to(self.device()),
            torch.unsqueeze(torch.tensor(action_history, dtype=torch.float32), 0).to(self.device()),
            torch.unsqueeze(torch.tensor(state_goal, dtype=torch.float32), 0).to(self.device()),
        )  

        # Get the optimal action plan and step it forward
        state, done_flag, done_message = self.environment.step(optimal_action_plan[0])

        # If we're done then reset
        if done_flag:
            self._environment_reset()

            # Only do one episode in validation
            if stage == "val":
                raise StopIteration(f"Validation environment is done, flag: {done_message}")
        
        # Note that after this call everything except log_p_likelihoods will be numpy.
        # log_p_likelihoods will be a torch tensor, with grad information

        # We'll do everything with log probabilities for stability
        # Costs. Note that we have the probability of a trajectory being optimal 
        # is estimated by the exponential of the negative cost: p(o|tau) = exp(-cost(tau)),
        # meaning that the log probability of optimality is -cost(tau)
        costs = torch.tensor(costs, dtype=torch.float32)
        costs = costs.to(log_p_likelihoods.device)
        log_p_optimality = -costs
        log_p_optimality = log_p_optimality.view(B, self.K)

        # Assert same shapes
        assert log_p_optimality.shape == log_p_likelihoods.shape, f"Expected log_p_optimality and log_p_likelihoods to have the same shape, got {log_p_optimality.shape} and {log_p_likelihoods.shape} respectively"

        # We now ensure that both the log likelihoods and the log optimality when converted into 
        # probability space would sum to 1, as we want to balance the two and ensure they are on
        # the same scale, as well as ensure that they represent true probability distributions
        def _make_log_probs_valid_probability_distribution(log_probs):
            # Use torch.logsumexp for numerically stable normalization
            log_probs_normalized = log_probs - torch.logsumexp(log_probs, dim=1, keepdim=True)
            return log_probs_normalized
        log_p_optimality  = _make_log_probs_valid_probability_distribution(log_p_optimality)
        log_p_likelihoods = _make_log_probs_valid_probability_distribution(log_p_likelihoods)

        # Here we define some hyperparameters that balance exploration and exploitation
        # exploitation refers to optimality, and exploration refers to the entropy of the
        # action distribution
        # If the following variable is large, then the probability of optimality is 
        # considered more so than the spread of the action distribution when computing
        # the loss
        # i.e. if this is big our distribution is dialed in on the optimal trajectory
        #      if this is small our distribution is more spread out, and less concerned with optimality
        prefer_optimality_over_exploration = 1

        # Addition in log space is multiplication in normal space
        # Multiplication in log space is exponentiation in normal space
        log_p_combined = prefer_optimality_over_exploration * log_p_optimality + log_p_likelihoods

        # What fraction of each samples combined probability does
        # each sample take up?
        # In normal space this would be p_combined / mean(p_combined)
        # In log space this becomes log(p_combined) - log(mean(p_combined))
        # Note that softmax is = exp(x) / sum(exp(x)), so when x = log_p_combined
        # we end up converting back to normal space and doing the normal space
        # calculation
        # Recall that dim here makes every slice along the dim sum to 1, so this
        # needs to be across all samples
        # NOTE we have a temperature parameter:
        # if T > 1, then we have a flatter, more uniform distribution
        # if T < 1, then we have a sharper, more peaked distribution
        temperature = 100
        weights = torch.softmax(log_p_combined / temperature, dim=1)

        # Assert the shape of weights
        assert weights.shape == (B, self.K), f"Expected weights to have shape (B={B}, K={self.K}), got {weights.shape}"
        # Check for nans
        assert not torch.isnan(weights).any(), f"Weights contain {torch.sum(torch.isnan(weights)).item()}/{B*self.K} NaNs"

        # Assemble the total loss
        log_p_likelihood_weighted_losses = -weights * log_p_likelihoods
        loss = torch.mean(log_p_likelihood_weighted_losses)

        # An alternative approach to the loss is to only keep the samples that are 
        # beneficial - then do the mean log likelihood of ONLY THOSE samples
        # loss = -flow.log_prob(inputs=x).mean()
        # TODO select the:
        # X% least cost 
        # X% most explorative samples (the ones that best explore state space)
        def score_cost(index):
            return costs[index]
        def score_exploration(index):
            # Return the length in positional space by looking at the state plans
            # and computing the distance along the path
            state_plan = state_plans[index]
            return torch.sum(torch.linalg.norm(state_plan[1:] - state_plan[:-1], dim=1))
        percentage_taken = 0.5
        num_samples_to_take = int(percentage_taken * self.K)
        # Get the least cost and most explorative samples
        def get_best_samples(score_fn):
            scores = score_fn(torch.arange(self.K))
            sorted_indices = torch.argsort(scores)
            return sorted_indices[:num_samples_to_take]
        best_cost_indices = get_best_samples(score_cost)
        best_exploration_indices = get_best_samples(score_exploration)
        # Combine the two
        best_indices = torch.unique(torch.cat((best_cost_indices, best_exploration_indices)))
        # Compute the loss
        loss = -torch.mean(log_p_likelihoods[best_indices])

        # Log the probability distributions in probability space, to do this we exponentiate
        # and normalize and move to cpu
        log_probability_plots = False 
        if batch_idx % 50 == 0 and log_probability_plots:
            self._log_probability_distributions(
                f"{stage}/probs_and_weights", 
                log_p_optimality, 
                log_p_likelihoods, 
                log_p_combined, 
                weights,
                log_p_likelihood_weighted_losses,
            )

        # Do custom logging
        self._log_scalar_per_step(f'{stage}/loss', loss, prog_bar=True)
        self._log_scalar_per_step(f'{stage}/cost/min', torch.min(costs), prog_bar=True)
        self._log_scalar_per_step(f'{stage}/cost/mean', torch.mean(costs))
        #self._log_scalar_per_step(f'{stage}/cost/max', torch.max(costs))

        # Double check that the task is changing during training
        # Loggers like to only log tensors, so cast before passing
        dist_initial_to_goal = torch.tensor(np.linalg.norm(state_goal[0:3] - state_initial[0:3]), dtype=torch.float32)
        self._log_scalar_per_step(f'{stage}/dist_initial_to_goal', dist_initial_to_goal, prog_bar=True)

        # And return the loss
        return loss
    
    def _log_scalar_per_step(self, name, value, prog_bar=False):
        B = 1
        self.log(name, value, on_step=True, on_epoch=True, prog_bar=prog_bar, logger=True, batch_size=B)

    def _log_probability_distributions(self, title, log_p_optimality, log_p_likelihoods, log_p_combined, weights, log_p_likelihood_weighted_losses):
        # Define an inner function to avoid code repetition
        def plot_distribution(ax, values, color, xlabel, ylabel):
            values = values.detach().cpu().numpy().squeeze()
            assert len(values.shape) == 1, f"Expected values to be 1D, got shape={values.shape}"
            # No gaps between bars
            ax.bar(range(len(values)), values, color=color, width=1.0)
            ax.set_ylabel(ylabel)
            ax.set_xlabel(xlabel)
            ax.set_xticks([])  # Hide x-ticks
            # Squeeze x axis
            ax.set_xlim(-0.5, len(values) - 0.5)

        # Create a figure with four subplots stacked vertically
        fig, axs = plt.subplots(5, 1, figsize=(14, 6))

        # Plot the distributions using the inner function
        plot_distribution(axs[0], log_p_optimality, 'blue', '(more negative=relatively higher cost=less optimal)', 'Log\nOptimality')
        plot_distribution(axs[1], log_p_likelihoods, 'red', '(more negative=less likely=more explorative & closer to zero=more likely=more exploitative)', 'Log\nLikelihood')
        plot_distribution(axs[2], log_p_combined, 'purple', '(more negative=less multiobjective desirability=lower weight)', 'Log\nCombined')
        plot_distribution(axs[3], weights, 'black', '(larger=more contribution to loss=more penalized)', 'Weights')
        plot_distribution(axs[4], log_p_likelihood_weighted_losses, 'black', '(larger=associated with more penalized behavior)', 'Log Likelihood\nWeighted Losses')

        plt.tight_layout()  # Adjust layout to fit everything nicely

        # TODO move this approach to utilities
        # Create a temporary file to save the plot, with delete=True
        with tempfile.NamedTemporaryFile(delete=True, suffix='.png') as tmpfile:
            # Save the figure
            plt.savefig(tmpfile.name)

            # Log the figure as an image to Wandb before the file gets deleted
            self.logger.experiment.log({title: wandb.Image(tmpfile.name)})

        # Close the plot to free up memory
        plt.close()

    def training_step(self, batch, batch_idx):
        return self.generic_step(batch, batch_idx, 'train')

    def validation_step(self, batch, batch_idx):
        return self.generic_step(batch, batch_idx, 'val')

    def on_validation_epoch_start(self):
        self.update_logging_enabled(True)

        # And reset the environment (the training environment may not have finished,
        # as opposed to the validation environment which MUST finish)
        self._environment_reset()

    def on_validation_epoch_end(self):

        # If the log folder does not have more than 10 steps in it, return (i.e. when we're doing
        # validation sanity checks)
        if len(os.listdir(self.policy_mppi_folder)) < 10:
            warnings.warn("Not enough steps to create a video - are you running a validation sanity check?")
        
        else:

            # Create visualization
            visual = Visual(self.log_folder)
            #visual.plot_histories()
            video_filepath = visual.render_video(desired_fps=25)

            # Rename the video to include the current timestamp
            timestamp = utils.general.get_timestamp(ultra_precise=True)
            video_filepath_new = video_filepath.replace(".mp4", f"_{timestamp}.mp4")
            os.rename(video_filepath, video_filepath_new)

            # Upload the video to wandb and delete the file
            wandb.log({"video": wandb.Video(video_filepath_new)})
            #os.remove(video_filepath)

        # Delete the logs
        self.delete_logs()

        # Don't log outside of validation
        self.update_logging_enabled(False)
        
    def configure_optimizers(self):
        learnable_parameters = self.model.parameters()
        return optim.Adam(learnable_parameters, lr=self.learning_rate)