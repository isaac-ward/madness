import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
import pytorch_lightning as pl

import nflows.nn.nets
from nflows.distributions.normal import StandardNormal
from nflows.flows import Flow
from nflows.transforms import CompositeTransform, ReversePermutation
from nflows.transforms.coupling import AffineCouplingTransform
from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform

import policies.costs
from policies.mppi import MPPIComputer
import policies.samplers

class PolicyFlowActionDistribution(pl.LightningModule):
    """
    Learns the optimal action distribution for MPPI using normalizing flows

    When used, an object of this class will be used in tandem with a MPPI policy 
    object and this will be used to generate optimal actions
    """
    def __init__(
        self, 
        dynamics,
        K,
        H,
        lambda_,
        map_,
        context_input_size,
        context_output_size,
        num_flow_layers,
        learning_rate,
    ):
        # Initialize the parent class (pl.LightningModule)
        super(PolicyFlowActionDistribution, self).__init__()

        # Don't defaultly set a goal, this needs to be set 
        self.state_goal = None

        # Save the learning rate
        self.learning_rate = learning_rate

        # Will need access to an MPPI computer
        self.mppi_computer = MPPIComputer(
            dynamics=dynamics,
            K=K,
            H=H,
            lambda_=lambda_,
            map_=map_,
        )

        # Define the internal shapes
        self.K = K
        self.H = H
        self.context_input_size = context_input_size
        self.context_output_size = context_output_size
        self.num_flow_layers = num_flow_layers

        # Create the learning network
        self.model = self._create_model()
        
    def update_state_goal(
        self,
        state_goal,
    ):
        self.state_goal = state_goal

    def _create_model(self):
        """
        Define a normalizing flow network that takes a sample from a 
        standard normal distribution and transforms it into a sample
        from the learned optimal action distribution

        When we sample from the learned optimal action distribution,
        we get a tensor with enough material in it to reshape into
        (H, |A|), where H is the horizon and |A| is the dimension of 
        the action space

        The flow should be parameterized by a fixed length context vector
        which is generated by a separate context network
        """

        # Convienience and readability
        A = self.mppi_computer.dynamics.action_size()
        H = self.H
        C = self.context_output_size
        #print(f"Flow network: A={A}, H={H}, C={C}")

        def create_net(in_features, out_features):
            # Make a little baby resnet
            net = nflows.nn.nets.ResidualNet(
                in_features,
                out_features, 
                context_features=C,
                hidden_features=128, 
                num_blocks=3,
                use_batch_norm=False,
            )
            return net

        transforms = []
        for _ in range(self.num_flow_layers):
            # https://github.com/bayesiains/nflows/blob/3b122e5bbc14ed196301969c12d1c2d94fdfba47/nflows/transforms/coupling.py#L212
            affine_coupling_transform = AffineCouplingTransform(
                # A simple alternating binary mask stating which features will be transformed
                # via an affine transformation (mask > 0) and which will inform the creation of the
                # the transformation parameters (mask <= 0, identity)
                mask=torch.arange(H*A) % 2,  
                # A function that creates a transformation object that which is constructed with two
                # constructor variables: the number of identity features, and the number of transform features
                # Moreover, it must have a forward function which accepts the parts of the input that are to 
                # be transformed, and the context vector, and produces some transformation parameters that will
                # be a
                #transform_net_create_fn=self._create_transform_parameters_block,
                transform_net_create_fn=create_net,
            )
            transforms.append(affine_coupling_transform)

            # Reverses the elements of a 1d input
            # https://github.com/bayesiains/nflows/blob/3b122e5bbc14ed196301969c12d1c2d94fdfba47/nflows/transforms/permutations.py#L56
            reverse_permutation = ReversePermutation(features=H*A)
            transforms.append(reverse_permutation)

        # Need a context network
        context_network = nn.Sequential(
            nn.Linear(self.context_input_size, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, self.context_output_size)
        )

        # Convert into a single composite transform
        transform = CompositeTransform(transforms)
        distribution = StandardNormal(shape=[H*A])
        flow = Flow(transform, distribution, embedding_net=context_network)
        return flow
    
    def forward(self, state_history, action_history, state_goal):

        # Convienience variables
        B = state_history.shape[0]
        S = self.mppi_computer.dynamics.state_size()
        A = self.mppi_computer.dynamics.action_size()
        H = self.H
        K = self.K
        C = self.context_output_size

        # Assert that the histories and goals have a batch dimension
        assert (state_history.shape[0] == B and action_history.shape[0] == B and state_goal.shape[0] == B), "Expected batch dimension to be first and equal for all inputs"    
        assert state_history.shape == (B, H, S), f"Expected state_history to have shape (B={B}, H={H}, |S|={S}), got {state_history.shape}"
        assert action_history.shape == (B, H-1, A), f"Expected action_history to have shape (B={B}, H={H-1}, |A|={A}), got {action_history.shape}"
        assert state_goal.shape == (B, S), f"Expected state_goal to have shape (B={B}, |S|={S}), got {state_goal.shape}"

        # Tensorize
        state_history   = torch.tensor(state_history, dtype=torch.float32)
        action_history  = torch.tensor(action_history, dtype=torch.float32)
        state_goal      = torch.tensor(state_goal, dtype=torch.float32)

        # Assemble the context vector, which includes the
        # current state
        # goal state
        state_current = state_history[:,-1,:]
        context_input = torch.cat((state_current, state_goal), dim=-1)

        # The context shape is (batch_size, context_size) at this point
        assert context_input.shape == (B, self.context_input_size), f"Expected context vector to have shape (B={B}, |C|={C}), got {context_input.shape}"

        # TODO there could be something cool here with multiple context vectors
        # representing different things

        # Sample from the normalizing flow and get log probs (this is more
        # efficient than sampling and then computing the log probs, though log
        # likelihoods are not needed during validation)
        # We'll get actions as (batch_size, K, H*A) 
        # We'll get log_likelihoods as (batch_size, K)
        # Read this as 'generate K samples for each context vector', where we only have one
        # context vector
        samples, log_likelihoods = self.model.sample_and_log_prob(K, context=context_input)

        # Do some checks
        assert samples.shape == (B, K, H*A), f"Expected action_plans to have shape (B={B}, K={K}, H*|A|={H*A}), got {samples.shape}"
        assert log_likelihoods.shape == (B, K), f"Expected log_likelihoods to have shape (B={B}, K={K}), got {log_likelihoods.shape}"

        # Reshape the batch of samples into B lots of K, H-long sequences of action vectors
        action_plans = samples.view(B, K, H, A)

        # We can't assume that dynamics and rewards are differentiable
        # TODO

        return action_plans, log_likelihoods
    
    def act(
        self,
        state_history,
        action_history,
    ):
        # Check if we have a goal
        if self.state_goal is None:
            raise ValueError("A goal state must be set before acting")
        
        # Return the optimal action
        action_plans, log_likelihoods = self.forward(
            # Batch size of 1
            np.expand_dims(state_history, axis=0),
            np.expand_dims(action_history, axis=0),
            np.expand_dims(self.state_goal, axis=0),
        )

        # Compute the optimal future using MPPI
        state_plans, action_plans, rewards, optimal_state_plan, optimal_action_plan = self.mppi_computer.compute(
            state_history[0],
            action_history[0],
            self.state_goal[0],
            # The MPPI computer needs to take a 'sampler'
            policies.samplers.FixedSampler(action_plans),
        )        

        return optimal_action_plan[0]
    
    def generic_step(self, batch, batch_idx, stage):
        """
        Generate K samples from the learned optimal action distribution,
        and compute a loss that encourages the samples to be close to
        the optimal action distribution
        """

        # Start by getting the contextual variables
        state_history = batch["state_history"]
        action_history = batch["action_history"]
        state_goal = batch["state_goal"]

        # TODO Replace with act

        # Run a pass, getting all the required information 
        # from the MPPI computer
        action_plans, log_likelihoods = self.forward(state_history, action_history, state_goal)

        # To compute rewards we need to roll forward dynamics and compute rewards,
        # and both of these would need to be backpropagated through
        # TODO

        # Things need to be tensors and on the same device
        rewards = torch.tensor(rewards, dtype=torch.float32).detach().cpu()
        log_likelihoods = torch.tensor(log_likelihoods, dtype=torch.float32).detach().cpu()

        # What was the average and best reward?
        reward_best = torch.max(rewards)
        reward_mean = torch.mean(rewards)

        # Compute the probabilities of optimality from the reward function
        # This is just the exponential of the rewards
        p_opt = torch.exp(rewards)

        # Compute the likelihood of the control sequences wrt the context vector
        # using the reverse mode of the normalizing flow
        p_likelihood = torch.exp(log_likelihoods)

        # TODO hyperparameter weighting of opt/entropy

        # Compute the loss weight for each sample
        # Combine probabilities elementwise and then get the mean
        p_combined = torch.mean(p_opt * p_likelihood)
        # What fraction of each samples combined probability does
        # each sample take up?
        weights = (p_opt * p_likelihood) / p_combined

        # Assemble the total loss
        loss = -torch.mean(weights * log_likelihoods)

        # Do custom logging
        self.log(f'{stage}/loss',        loss,        on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.log(f'{stage}/reward/best', reward_best, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.log(f'{stage}/reward/mean', reward_mean, on_step=True, on_epoch=True, prog_bar=True, logger=True)

        # And return the loss
        return loss
    
    def training_step(self, batch, batch_idx):
        return self.generic_step(batch, batch_idx, 'train')

    def validation_step(self, batch, batch_idx):
        return self.generic_step(batch, batch_idx, 'val')
        
    def configure_optimizers(self):
        learnable_parameters = self.model.parameters()
        return optim.Adam(learnable_parameters, lr=self.learning_rate)