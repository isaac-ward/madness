import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
import pytorch_lightning as pl

import nflows.nn.nets
from nflows.distributions.normal import StandardNormal
from nflows.flows import Flow
from nflows.transforms import CompositeTransform, ReversePermutation
from nflows.transforms.coupling import AffineCouplingTransform
from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform

import policies.costs
from policies.mppi import MPPIComputer
import policies.samplers

class PolicyFlowActionDistribution(pl.LightningModule):
    """
    Learns the optimal action distribution for MPPI using normalizing flows

    When used, an object of this class will be used in tandem with a MPPI policy 
    object and this will be used to generate optimal actions
    """
    def __init__(
        self, 
        dynamics,
        K,
        H,
        lambda_,
        map_,
        context_input_size,
        context_output_size,
        num_flow_layers,
        learning_rate,
    ):
        # Initialize the parent class (pl.LightningModule)
        super(PolicyFlowActionDistribution, self).__init__()

        # Don't defaultly set a goal, this needs to be set 
        self.state_goal = None

        # Save the learning rate
        self.learning_rate = learning_rate

        # Will need access to an MPPI computer
        self.mppi_computer = MPPIComputer(
            dynamics=dynamics,
            K=K,
            H=H,
            lambda_=lambda_,
            map_=map_,
        )

        # Define the internal shapes
        self.K = K
        self.H = H
        self.context_input_size = context_input_size
        self.context_output_size = context_output_size
        self.num_flow_layers = num_flow_layers

        # Create the learning network
        self.model = self._create_model()
        
    def update_state_goal(
        self,
        state_goal,
    ):
        self.state_goal = state_goal

    def _create_model(self):
        """
        Define a normalizing flow network that takes a sample from a 
        standard normal distribution and transforms it into a sample
        from the learned optimal action distribution

        When we sample from the learned optimal action distribution,
        we get a tensor with enough material in it to reshape into
        (H, |A|), where H is the horizon and |A| is the dimension of 
        the action space

        The flow should be parameterized by a fixed length context vector
        which is generated by a separate context network
        """

        # Convienience and readability
        A = self.mppi_computer.dynamics.action_size()
        H = self.H
        C = self.context_output_size
        #print(f"Flow network: A={A}, H={H}, C={C}")

        def create_net(in_features, out_features):
            # Make a little baby resnet
            net = nflows.nn.nets.ResidualNet(
                in_features,
                out_features, 
                context_features=C,
                hidden_features=128, 
                num_blocks=3,
                use_batch_norm=False,
            )
            return net

        transforms = []
        for _ in range(self.num_flow_layers):
            # https://github.com/bayesiains/nflows/blob/3b122e5bbc14ed196301969c12d1c2d94fdfba47/nflows/transforms/coupling.py#L212
            affine_coupling_transform = AffineCouplingTransform(
                # A simple alternating binary mask stating which features will be transformed
                # via an affine transformation (mask > 0) and which will inform the creation of the
                # the transformation parameters (mask <= 0, identity)
                mask=torch.arange(H*A) % 2,  
                # A function that creates a transformation object that which is constructed with two
                # constructor variables: the number of identity features, and the number of transform features
                # Moreover, it must have a forward function which accepts the parts of the input that are to 
                # be transformed, and the context vector, and produces some transformation parameters that will
                # be a
                #transform_net_create_fn=self._create_transform_parameters_block,
                transform_net_create_fn=create_net,
            )
            transforms.append(affine_coupling_transform)

            # Reverses the elements of a 1d input
            # https://github.com/bayesiains/nflows/blob/3b122e5bbc14ed196301969c12d1c2d94fdfba47/nflows/transforms/permutations.py#L56
            reverse_permutation = ReversePermutation(features=H*A)
            transforms.append(reverse_permutation)

        # Need a context network
        context_network = nn.Sequential(
            nn.Linear(self.context_input_size, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, self.context_output_size)
        )

        # Convert into a single composite transform
        transform = CompositeTransform(transforms)
        distribution = StandardNormal(shape=[H*A])
        flow = Flow(transform, distribution, embedding_net=context_network)
        return flow
    
    def forward(self, state_history, action_history, state_goal):

        # Convienience variables
        B = state_history.shape[0]
        S = self.mppi_computer.dynamics.state_size()
        A = self.mppi_computer.dynamics.action_size()
        H = self.H
        L = state_history.shape[1] # Lookback
        K = self.K
        C = self.context_output_size

        # TODO: expand to handle batched inputs
        # Batch size should only ever be 1 at this time
        if B != 1:
            raise ValueError(f"Batch size must be 1, but is {B}")

        # Assert that the histories and goals have a batch dimension
        assert (state_history.shape[0] == B and action_history.shape[0] == B and state_goal.shape[0] == B), "Expected batch dimension to be first and equal for all inputs"    
        assert state_history.shape == (B, L, S), f"Expected state_history to have shape (B={B}, L={L}, |S|={S}), got {state_history.shape}"
        assert action_history.shape == (B, L-1, A), f"Expected action_history to have shape (B={B}, L={L-1}, |A|={A}), got {action_history.shape}"
        assert state_goal.shape == (B, S), f"Expected state_goal to have shape (B={B}, |S|={S}), got {state_goal.shape}"

        # Tensorize if they are not already tensors 
        if not torch.is_tensor(state_history):
            state_history = torch.tensor(state_history, dtype=torch.float32)
        if not torch.is_tensor(action_history):
            action_history = torch.tensor(action_history, dtype=torch.float32)
        if not torch.is_tensor(state_goal):
            state_goal = torch.tensor(state_goal, dtype=torch.float32)

        # Assemble the context vector, which includes the
        # current state
        # goal state
        context_input = torch.cat((
            state_history[:,-1,:], 
            state_goal
        ), dim=-1)

        # The context shape is (batch_size, context_size) at this point
        assert context_input.shape == (B, self.context_input_size), f"Expected context vector to have shape (B={B}, |C|={C}), got {context_input.shape}"

        # TODO there could be something cool here with multiple context vectors
        # representing different things

        # Ensure there are no nans in the context
        if torch.isnan(context_input).any():
            raise ValueError(f"Context vector contains NaNs")
        print(context_input)

        # Sample from the normalizing flow and get log probs (this is more
        # efficient than sampling and then computing the log probs, though log
        # likelihoods are not needed during validation)
        # We'll get actions as (batch_size, K, H*A) 
        # We'll get log_p_likelihoods as (batch_size, K)
        # Read this as 'generate K samples for each context vector', where we only have one
        # context vector
        samples, log_p_likelihoods = self.model.sample_and_log_prob(K, context=context_input)

        # Are there any nans in the result?
        if torch.isnan(samples).any():
            raise ValueError(f"Normalizing flow samples contain NaNs")
        if torch.isnan(log_p_likelihoods).any():
            raise ValueError(f"Normalizing flow log likelihoods contain NaNs")
        
        # Print the nyumber of nans
        num_nans = torch.sum(torch.isnan(samples)).item()
        print(f"Number of nans in samples: {num_nans}")

        # Do some checks
        assert samples.shape == (B, K, H*A), f"Expected action_plans to have shape (B={B}, K={K}, H*|A|={H*A}), got {samples.shape}"
        assert log_p_likelihoods.shape == (B, K), f"Expected log_p_likelihoods to have shape (B={B}, K={K}), got {log_p_likelihoods.shape}"

        # Reshape the batch of samples into B lots of K, H-long sequences of action vectors
        action_plans = samples.view(B, K, H, A)

        # To get a loss we'll need to see what trajectories these action plans
        # result it, and score them, but we can't assume that dynamics and 
        # rewards are differentiable, so we'll need to compute the trajectories
        # and scores without grads (grad information will still be present in the
        # conditional variational posterior)

        # Copy the action plans and numpy-ify them
        action_plans = action_plans[0].detach().cpu().numpy()

        # Make sure the action plans have no nans
        if np.isnan(action_plans).any():
            raise ValueError("Action plans contain NaNs")

        # Compute the optimal future using MPPI
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan = self.mppi_computer.compute(
            # Recall that batches are of size 1
            state_history[0].detach().cpu().numpy(),
            action_history[0].detach().cpu().numpy(),
            state_goal[0].detach().cpu().numpy(),
            # The MPPI computer needs to take a 'sampler'
            policies.samplers.FixedSampler(action_plans),
        )    

        return state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, log_p_likelihoods
    
    def act(
        self,
        state_history,
        action_history,
    ):
        # Check if we have a goal
        if self.state_goal is None:
            raise ValueError("A goal state must be set before acting")
        
        # Return the optimal action
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, log_likelihoods = self.forward(
            # Batch size of 1
            np.expand_dims(state_history, axis=0),
            np.expand_dims(action_history, axis=0),
            np.expand_dims(self.state_goal, axis=0),
        )    

        return optimal_action_plan[0]
    
    def generic_step(self, batch, batch_idx, stage):
        """
        Generate K samples from the learned optimal action distribution,
        and compute a loss that encourages the samples to be close to
        the optimal action distribution
        """

        # Start by getting the contextual variables
        state_history = batch["state_history"]
        action_history = batch["action_history"]
        state_goal = batch["state_goal"]

        # What is the batch size?
        B = state_history.shape[0]

        # Run a pass, getting all the required information 
        # from the MPPI computer
        state_plans, action_plans, costs, optimal_state_plan, optimal_action_plan, log_p_likelihoods = self.forward(
            # Batch size of 1
            state_history,
            action_history,
            state_goal
        )  
        
        # Note that after this call everything except log_p_likelihoods will be numpy.
        # log_p_likelihoods will be a torch tensor, with grad information

        # We'll do everything with log probabilities for stability
        # Costs. Note that we have the probability of a trajectory being optimal 
        # is estimated by the exponential of the negative cost: p(o|tau) = exp(-cost(tau)),
        # meaning that the log probability of optimality is -cost(tau)
        costs = torch.tensor(costs, dtype=torch.float32)
        costs = costs.to(log_p_likelihoods.device)
        log_p_optimality = -costs

        # Addition in log space is multiplication in normal space
        log_p_combined = log_p_optimality + log_p_likelihoods

        # What fraction of each samples combined probability does
        # each sample take up?
        # In normal space this would be p_combined / mean(p_combined)
        # In log space this becomes log(p_combined) - log(mean(p_combined))
        # Note that softmax is = exp(x) / sum(exp(x)) which is proportionally correct
        # TODO check this
        weights = torch.softmax(log_p_combined, dim=0)

        # Assemble the total loss
        loss = -torch.mean(weights * log_p_likelihoods)

        # Ensure non NaNs anywhere
        if torch.isnan(p_opt).any():
            raise ValueError(f"p_opt contains {torch.sum(torch.isnan(p_opt)).item()} NaNs")
        if torch.isnan(p_likelihood).any():
            raise ValueError(f"p_likelihood contains {torch.sum(torch.isnan(p_likelihood)).item()} NaNs")
        if torch.isnan(p_combined).any():
            raise ValueError(f"p_combined contains {torch.sum(torch.isnan(p_combined)).item()} NaNs")
        print("Probability distributions")
        print(-costs)
        print(log_likelihoods)
        print(p_opt)
        print(p_likelihood)
        print(p_combined)
        if torch.isnan(weights).any():
            raise ValueError(f"Weights contain {torch.sum(torch.isnan(weights)).item()} NaNs")
        if torch.isnan(log_likelihoods).any():
            raise ValueError(f"Log likelihoods contain {torch.sum(torch.isnan(log_likelihoods)).item()} NaNs")
        if torch.isnan(loss).any():
            raise ValueError(f"Loss contains {torch.sum(torch.isnan(loss)).item()} NaNs")

        # Do custom logging
        self._log_scalar_per_step(f'{stage}/loss', loss)
        self._log_scalar_per_step(f'{stage}/cost/min', torch.min(costs))
        self._log_scalar_per_step(f'{stage}/cost/mean', torch.mean(costs))
        self._log_scalar_per_step(f'{stage}/cost/max', torch.max(costs))

        # And return the loss
        return loss
    
    def _log_scalar_per_step(self, name, value):
        B = 1
        self.log(name, value, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=B)
    
    def training_step(self, batch, batch_idx):
        return self.generic_step(batch, batch_idx, 'train')

    def validation_step(self, batch, batch_idx):
        return self.generic_step(batch, batch_idx, 'val')
        
    def configure_optimizers(self):
        learnable_parameters = self.model.parameters()
        return optim.Adam(learnable_parameters, lr=self.learning_rate)